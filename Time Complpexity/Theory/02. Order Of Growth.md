## Order Of Growth

When comparing two functions `f(n)` and `g(n)` we always assume that   
 n >= 0 ,  
 Time taken >= 0,    
 f(n), g(n) >= 0.

- A function `f(n)` is said to be growing faster than `g(n)` if 
```
lim     f(n)
n -> ∞  ---- = ∞
        g(n)

        OR

lim     g(n)
n -> ∞  ---- = 0
        f(n)        
```

Example: 
```
f(n) = n^2 + 2n+ 5
g(n) = 2n + 5

lim     g(n)          2n + 5               
n -> ∞  ---- = 0  =  --------        =  0 
        f(n)          n^2 + 2n + 5
```
Take the highest order term and divide both numerator and denominator with that in this case it's n^2. 

Hence this implies that `f(n)` is growing faster. 

There is also a way to directly compare the complexity of two functions. 
       -- Ignore the lower order terms  
       -- Ignore the leading constants

Example : 
```
f(n) = 2n^2 + n + 5 
g(n) = 100n + 5;

// Removing terms lower than n^2 as thats the highest power. 

f(n) = n^2
g(n) = n
```
Hence f(n) grows faster. 

### Ordering of terms
```
c < log logn < log n < n logn < n ^ 1/3 < n ^ 1/2 < n < n^2 < n^3 < n^4 < 2^n < n^n 
```

The order of growth, also known as time complexity or computational complexity, is a measure of how the running time of an algorithm or the growth of a function scales as the input size increases. It helps us understand how the algorithm's performance changes when the input grows larger.

Order of growth is typically expressed using Big O notation. Big O notation provides an upper bound on the growth rate of a function. For a given algorithm or function, we denote its order of growth as O(f(n)), where "f(n)" represents the growth rate function and "n" represents the input size.

Here are some commonly encountered orders of growth, listed in increasing order of their growth rates:

1. O(1) - Constant time complexity: The algorithm's running time remains constant, regardless of the input size. It is considered the most efficient order of growth.

2. O(log n) - Logarithmic time complexity: The running time increases logarithmically with the input size. Algorithms with this complexity often divide the input in half at each step, such as binary search.

3. O(n) - Linear time complexity: The running time grows linearly with the input size. As the input size doubles, the running time also doubles. Many simple search or traversal algorithms fall into this category.

4. O(n log n) - Linearithmic time complexity: The running time grows in proportion to the product of the input size and its logarithm. Sorting algorithms like quicksort and mergesort often exhibit this complexity.

5. O(n^2) - Quadratic time complexity: The running time grows quadratically with the input size. Algorithms that involve nested loops are common examples of this complexity.

6. O(n^c) - Polynomial time complexity: The running time grows as a polynomial function of the input size, where "c" represents a constant exponent. Higher values of "c" indicate worse performance. For example, O(n^3) represents cubic time complexity.

7. O(2^n) - Exponential time complexity: The running time doubles with each increment in the input size. Algorithms with exponential complexity are highly inefficient and generally become impractical for large inputs.

